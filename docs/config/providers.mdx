---
title: Providers
description: Configure API keys and settings for AI providers
---

mux supports multiple AI providers. The easiest way to configure them is through **Settings → Providers** (`Cmd+,` / `Ctrl+,`).

## Quick Setup

1. Open Settings (`Cmd+,` / `Ctrl+,`)
2. Navigate to **Providers**
3. Expand any provider and enter your API key
4. Start using models from that provider

Most providers only need an API key. The UI handles validation and shows which providers are configured.

## Supported Providers

| Provider          | Models                     | Get API Key                                             |
| ----------------- | -------------------------- | ------------------------------------------------------- |
| **Anthropic**     | Claude Opus, Sonnet, Haiku | [console.anthropic.com](https://console.anthropic.com/) |
| **Azure Foundry** | Claude via Azure Foundry   | [ai.azure.com](https://ai.azure.com/)                   |
| **OpenAI**        | GPT-5, Codex               | [platform.openai.com](https://platform.openai.com/)     |
| **Google**        | Gemini Pro, Flash          | [aistudio.google.com](https://aistudio.google.com/)     |
| **xAI**           | Grok                       | [console.x.ai](https://console.x.ai/)                   |
| **DeepSeek**      | DeepSeek Chat, Reasoner    | [platform.deepseek.com](https://platform.deepseek.com/) |
| **OpenRouter**    | 300+ models                | [openrouter.ai](https://openrouter.ai/)                 |
| **Ollama**        | Local models               | [ollama.com](https://ollama.com/) (no key needed)       |
| **Bedrock**       | Claude via AWS             | AWS Console                                             |

## Environment Variables

Providers also read from environment variables as fallback:

{/* BEGIN PROVIDER_ENV_VARS */}

| Provider      | Environment Variable                                  |
| ------------- | ----------------------------------------------------- |
| Anthropic     | `ANTHROPIC_API_KEY` or `ANTHROPIC_AUTH_TOKEN`         |
| Azure Foundry | `AZURE_FOUNDRY_API_KEY` and `AZURE_FOUNDRY_ENDPOINT` |
| OpenAI        | `OPENAI_API_KEY`                                      |
| Google        | `GOOGLE_GENERATIVE_AI_API_KEY` or `GOOGLE_API_KEY`   |
| xAI           | `XAI_API_KEY`                                         |
| OpenRouter    | `OPENROUTER_API_KEY`                                  |
| DeepSeek      | `DEEPSEEK_API_KEY`                                    |
| Bedrock       | `AWS_REGION` (credentials via AWS SDK chain)          |

<details>
<summary>Additional environment variables</summary>

| Provider     | Variable                   | Purpose             |
| ------------ | -------------------------- | ------------------- |
| Anthropic    | `ANTHROPIC_BASE_URL`       | Custom API endpoint |
| OpenAI       | `OPENAI_BASE_URL`          | Custom API endpoint |
| OpenAI       | `OPENAI_ORG_ID`            | Organization ID     |
| Google       | `GOOGLE_BASE_URL`          | Custom API endpoint |
| xAI          | `XAI_BASE_URL`             | Custom API endpoint |
| Azure OpenAI | `AZURE_OPENAI_API_KEY`     | API key             |
| Azure OpenAI | `AZURE_OPENAI_ENDPOINT`    | Endpoint URL        |
| Azure OpenAI | `AZURE_OPENAI_DEPLOYMENT`  | Deployment name     |
| Azure OpenAI | `AZURE_OPENAI_API_VERSION` | API version         |

Azure OpenAI env vars configure the OpenAI provider with Azure backend.

</details>

{/* END PROVIDER_ENV_VARS */}

## Advanced: Manual Configuration

For advanced options not exposed in the UI, edit `~/.mux/providers.jsonc` directly:

```jsonc
{
  "anthropic": {
    "apiKey": "sk-ant-...",
    "baseUrl": "https://api.anthropic.com", // Optional custom endpoint
  },
  "openrouter": {
    "apiKey": "sk-or-v1-...",
    // Provider routing preferences
    "order": ["Cerebras", "Fireworks"],
    "allow_fallbacks": true,
  },
  "xai": {
    "apiKey": "sk-xai-...",
    // Search orchestration settings
    "searchParameters": { "mode": "auto" },
  },
  "bedrock": {
    "region": "us-east-1",
    // Uses AWS credential chain if no explicit credentials
  },
  "ollama": {
    "baseUrl": "http://your-server:11434/api", // Custom Ollama server
  },
}
```

### Azure Foundry

Azure Foundry provides access to Claude models through Microsoft's AI marketplace. It uses Anthropic's native API format.

```jsonc
{
  "azure-foundry": {
    "apiKey": "your-azure-api-key",
    "baseURL": "https://your-resource.services.ai.azure.com/anthropic"
  }
}
```

**Getting your credentials:**
1. Go to [Azure AI Foundry](https://ai.azure.com/)
2. Create a project and deploy a Claude model
3. Copy your API key and endpoint URL
4. Configure mux with the endpoint (must end with `/anthropic`, NOT `/anthropic/v1`)

**Environment variables:**
```bash
export AZURE_FOUNDRY_ENDPOINT=https://your-resource.services.ai.azure.com/anthropic
export AZURE_FOUNDRY_API_KEY=your-azure-api-key
```

**Important:** The endpoint should be `https://{resource}.services.ai.azure.com/anthropic` (without `/v1` suffix). The SDK automatically appends API paths.

**Note:** Azure Foundry is separate from Azure OpenAI. All Claude features work identically: streaming, tool calling, thinking, and prompt caching.

### Bedrock Authentication

Bedrock supports multiple authentication methods (tried in order):

1. **Bearer Token** — Single API key via `bearerToken` config or `AWS_BEARER_TOKEN_BEDROCK` env var
2. **Explicit Credentials** — `accessKeyId` + `secretAccessKey` in config
3. **AWS Credential Chain** — Automatic resolution from environment, `~/.aws/credentials`, SSO, EC2/ECS roles

If you're already authenticated with AWS CLI (`aws sso login`), mux uses those credentials automatically.

### OpenRouter Provider Routing

Control which infrastructure providers handle your requests:

- `order`: Priority list of providers (e.g., `["Cerebras", "Fireworks"]`)
- `allow_fallbacks`: Whether to try other providers if preferred ones are unavailable
- `only` / `ignore`: Restrict or exclude specific providers
- `data_collection`: `"allow"` or `"deny"` for training data policies

See [OpenRouter Provider Routing docs](https://openrouter.ai/docs/features/provider-routing) for details.

### xAI Search Orchestration

Grok models support live web search. mux enables this by default with `mode: "auto"`. Customize via [`searchParameters`](https://docs.x.ai/docs/resources/search) for regional focus, time filters, or to disable search.
